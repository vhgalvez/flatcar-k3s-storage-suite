# playbooks/cleanup_longhorn.yml

- name: ğŸ’£ 1 - Eliminar Helm release de Longhorn (si existe)
  hosts: controller
  become: true
  gather_facts: false

  vars:
    kubeconfig_path: "/home/victory/.kube/config"
    helm_bin: "/usr/local/bin/helm"
    namespace: "longhorn-system"
    release_name: "longhorn"

  tasks:
    - name: ğŸ§¹ Desinstalar release de Helm si existe
      shell: |
        export KUBECONFIG={{ kubeconfig_path }}
        {{ helm_bin }} uninstall {{ release_name }} -n {{ namespace }} || true
      changed_when: true


- name: ğŸ§¼ 2 - Borrar recursos del namespace Longhorn + eliminar CRDs y namespace
  hosts: controller
  become: true
  gather_facts: false

  vars:
    kubeconfig_path: "/home/victory/.kube/config"
    kubectl_bin: "/usr/local/bin/kubectl"
    namespace: "longhorn-system"

  tasks:
    - name: ğŸ§ª Instalar jq si no estÃ¡ presente
      package:
        name: jq
        state: present

    - name: âŒ Borrar todos los recursos del namespace Longhorn
      shell: |
        export KUBECONFIG={{ kubeconfig_path }}
        {{ kubectl_bin }} delete all --all -n {{ namespace }} --ignore-not-found
        {{ kubectl_bin }} delete pvc --all -n {{ namespace }} --ignore-not-found
        {{ kubectl_bin }} delete pv --all --ignore-not-found
      changed_when: true

    - name: ğŸ’£ Eliminar CRDs Longhorn forzadamente con patch
      shell: |
        export KUBECONFIG={{ kubeconfig_path }}
        for crd in $({{ kubectl_bin }} get crd -o name | grep 'longhorn.io'); do
          name=$(basename "$crd")
          {{ kubectl_bin }} patch crd "$name" -p '{"metadata":{"finalizers":[]}}' --type=merge || true
          {{ kubectl_bin }} delete crd "$name" --grace-period=0 --force || true
        done
      register: crd_cleanup
      changed_when: true
      failed_when: false

    - name: ğŸ§¨ Verificar y eliminar finalizers del namespace si existen
      shell: |
        export KUBECONFIG={{ kubeconfig_path }}
        if {{ kubectl_bin }} get namespace {{ namespace }} -o json | jq -e '.spec.finalizers | length > 0' > /dev/null; then
          {{ kubectl_bin }} get namespace {{ namespace }} -o json | jq 'del(.spec.finalizers)' > /tmp/ns-clean.json
          {{ kubectl_bin }} replace --raw "/api/v1/namespaces/{{ namespace }}/finalize" -f /tmp/ns-clean.json || true
          rm -f /tmp/ns-clean.json
        fi
      changed_when: true
      failed_when: false

    - name: âŒ Eliminar el namespace completamente (con espera activa)
      shell: |
        export KUBECONFIG={{ kubeconfig_path }}
        {{ kubectl_bin }} delete namespace {{ namespace }} --ignore-not-found
        for i in $(seq 1 24); do
          sleep 5
          {{ kubectl_bin }} get namespace {{ namespace }} &>/dev/null || exit 0
        done
        echo "âš ï¸ Namespace sigue existiendo tras esperar 120s."
        exit 1
      register: ns_delete_result
      failed_when: ns_delete_result.rc != 0
      changed_when: true

    - name: ğŸš¨ Forzar eliminaciÃ³n final si aÃºn existe
      shell: |
        export KUBECONFIG={{ kubeconfig_path }}
        if {{ kubectl_bin }} get namespace {{ namespace }} &>/dev/null; then
          {{ kubectl_bin }} get namespace {{ namespace }} -o json | jq 'del(.spec.finalizers)' > /tmp/ns-force.json
          {{ kubectl_bin }} replace --raw "/api/v1/namespaces/{{ namespace }}/finalize" -f /tmp/ns-force.json || true
          rm -f /tmp/ns-force.json
        fi
      changed_when: true
      failed_when: false

    - name: ğŸ§¾ Mostrar resumen de limpieza de CRDs
      debug:
        var: crd_cleanup.stdout_lines


- name: ğŸ’½ 3 - Limpiar completamente /dev/vdb en storage y workers
  hosts: storage,workers
  become: true
  gather_facts: false

  tasks:
    - name: ğŸ” Mostrar si hay montajes activos en /dev/vdb
      raw: mount | grep vdb || true
      changed_when: false

    - name: ğŸ’£ Desmontar volÃºmenes
      raw: |
        umount -f /mnt/longhorn-disk 2>/dev/null || true
        umount -f /srv/nfs/postgresql 2>/dev/null || true
        umount -f /srv/nfs/shared 2>/dev/null || true
        sleep 2
      ignore_errors: true

    - name: â³ Esperar a que /dev/vdb ya no estÃ© montado
      raw: |
        for i in $(seq 1 5); do
          mount | grep -q vdb || exit 0
          sleep 1
        done
        exit 1
      register: umount_check
      failed_when: umount_check.rc != 0
      changed_when: false

    - name: âŒ Detener servicios que puedan bloquear el disco (solo en storage)
      raw: |
        systemctl stop nfs-server || true
        systemctl stop iscsid || true
      when: "'storage' in group_names"

    - name: ğŸ§¹ Eliminar volÃºmenes LVM (si existen)
      raw: |
        lvremove -f /dev/vg_storage/postgres_lv 2>/dev/null || true
        lvremove -f /dev/vg_storage/shared_lv 2>/dev/null || true
        lvremove -f /dev/vg_storage/longhorn_lv 2>/dev/null || true
        sleep 1

    - name: ğŸ§± Eliminar grupo de volÃºmenes
      raw: vgremove -f vg_storage 2>/dev/null || true

    - name: ğŸ’½ Eliminar volumen fÃ­sico
      raw: pvremove -f /dev/vdb1 2>/dev/null || true

    - name: â Eliminar particiÃ³n y limpiar firmas
      raw: |
        parted -s /dev/vdb rm 1 2>/dev/null || true
        wipefs -a /dev/vdb 2>/dev/null || true

    - name: ğŸ§¼ Esperar a que el sistema asiente los cambios
      raw: udevadm settle


# playbooks/04_deploy_longhorn.yml
--- 
- name: ğŸš€ 4 - Instalar Longhorn desde el nodo controller
  hosts: controller
  become: true
  gather_facts: false

  vars:
    kubeconfig_path: "/home/victory/.kube/config"
    kubectl_bin: "/usr/local/bin/kubectl"
    helm_bin: "/usr/local/bin/helm"
    longhorn_namespace: "longhorn-system"
    longhorn_release: "longhorn"

  pre_tasks:
    - name: ğŸ”§ Crear namespace Longhorn si no existe
      command: >
        {{ kubectl_bin }} --kubeconfig {{ kubeconfig_path }} create namespace {{ longhorn_namespace }}
      register: ns_create
      failed_when: ns_create.rc != 0 and 'AlreadyExists' not in ns_create.stderr
      changed_when: "'created' in ns_create.stdout"

    - name: âš ï¸ Verificar si el namespace tiene recursos
      command: >
        {{ kubectl_bin }} --kubeconfig {{ kubeconfig_path }} get all -n {{ longhorn_namespace }}
      register: longhorn_check
      failed_when: false
      changed_when: false

    - name: âŒ Abortamos si el namespace '{{ longhorn_namespace }}' tiene recursos
      fail:
        msg: |
          âš ï¸ El namespace '{{ longhorn_namespace }}' contiene recursos de una instalaciÃ³n anterior.
          ğŸ§¹ Para limpiar y reinstalar ejecuta:
            ansible-playbook playbooks/cleanup_longhorn.yml -i inventory/hosts.ini -e "confirm_cleanup=yes"
          Recursos detectados:
          {{ longhorn_check.stdout | default('') }}
      when:
        - longhorn_check.stdout is defined
        - longhorn_check.stdout | trim != ''
        - "'No resources found' not in longhorn_check.stdout"

  tasks:
    - name: ğŸ“¥ AÃ±adir repositorio Helm de Longhorn (si no existe)
      command: >
        {{ helm_bin }} repo add longhorn https://charts.longhorn.io
      register: helm_repo_add
      failed_when: false
      changed_when: "'already exists' not in helm_repo_add.stdout"

    - name: ğŸ”„ Actualizar repositorio Helm
      command: >
        {{ helm_bin }} repo update

    - name: ğŸš€ Instalar Longhorn con Helm
      command: >
        {{ helm_bin }} upgrade --install {{ longhorn_release }} longhorn/longhorn
        --namespace {{ longhorn_namespace }}
        --cleanup-on-fail
        --wait
        --timeout 10m0s
        --kubeconfig {{ kubeconfig_path }}
      register: longhorn_install
      failed_when: longhorn_install.rc != 0

    - name: ğŸ” Verificar si el release tiene recursos tras instalaciÃ³n
      shell: |
        export KUBECONFIG={{ kubeconfig_path }}
        {{ helm_bin }} get all {{ longhorn_release }} -n {{ longhorn_namespace }} || exit 1
      register: helm_get_result
      failed_when: helm_get_result.rc != 0 or 'MANIFEST' not in helm_get_result.stdout
      changed_when: false

    - name: â³ Esperar a que Longhorn cree al menos un pod (mÃ¡x 120s)
      shell: |
        for i in {1..24}; do
          {{ kubectl_bin }} --kubeconfig {{ kubeconfig_path }} get pods -n {{ longhorn_namespace }} --no-headers 2>/dev/null | grep -qv '^$' && exit 0
          sleep 5
        done
        echo "âŒ No se detectaron pods en '{{ longhorn_namespace }}'"
        {{ kubectl_bin }} --kubeconfig {{ kubeconfig_path }} get events -n {{ longhorn_namespace }} || true
        exit 1
      register: pod_exist_check
      failed_when: pod_exist_check.rc != 0
      changed_when: false

    - name: â³ Esperar a que los pods estÃ©n listos (mÃ¡x 5 minutos)
      shell: >
        {{ kubectl_bin }} --kubeconfig {{ kubeconfig_path }} wait --for=condition=Ready pod -n {{ longhorn_namespace }} --all --timeout=300s
      register: pod_ready
      failed_when: pod_ready.rc != 0

    - name: ğŸ“Š Mostrar estado de pods, PVCs y PVs
      shell: |
        echo "ğŸ“¦ Pods:"
        {{ kubectl_bin }} --kubeconfig {{ kubeconfig_path }} get pods -n {{ longhorn_namespace }}
        echo ""
        echo "ğŸ“ PVCs:"
        {{ kubectl_bin }} --kubeconfig {{ kubeconfig_path }} get pvc -A
        echo ""
        echo "ğŸ“¦ PVs:"
        {{ kubectl_bin }} --kubeconfig {{ kubeconfig_path }} get pv
      register: volumes_info

    - name: âœ… Resultado del despliegue
      debug:
        var: volumes_info.stdout_lines